---
title: "Equations"
author: "Eric Scott"
date: "5/10/2019"
output:
  word_document: default
  html_document:
    df_print: paged
---

## R^2^~X~

$R^2_X(cum)$: "Cumulative SS of all the Xs explained by all extracted components". From Eriksson et al.
**This is the same for PCA and PLS.**


$$
R^2_X(cum) = \sum 1-{RSS}/{SSX_{tot.corr.}}
$$


## R^2^~Y~

$R^2_Y(cum)$: "Cumulative SS of all the Ys explained by all extracted components"

**For PLS:**

$$
R^2_Y(cum) = \sum 1-{RSS}/{SSY_{tot.corr.}}
$$

**For PCA-DA:**
It's actually the coefficient of discrimination from Tjur 2009

$$
R^2_Y = D = \bar{\hat{\pi}}_1 - \bar{\hat{\pi}}_0
$$
Where $\bar{\hat{\pi}_1}$ and $\bar{\hat{\pi}}_0$ are mean fitted values for successes and failures.


## Q^2^

$Q^2(cum)$: "The fraction of the total variation of the Ys that can be predited by a component according to cross-validation"

$$
Q^2 = 1 - PRESS/SSY_{total}
$$

Predicted Error Sum of Squares (PRESS) is squared differences between observed Y and predicted values when observations were kept out (because of cross-validation).

$$
PRESS = \sum_{i}\sum_{m}(Y_{im} - \hat{Y}_{im})^2
$$

So...

$$
Q^2(cum) = (1 - \prod(PRESS/SS)_a) \;\;\;\; [a = 1,...A]
$$

where $\prod(PRESS/SS)_a$ = the product of PRESS/SS for each individual component a.

## Permutation p-values

$p_{Q^2}$ and $p_{R^2}$ are derived from permutation testing and are the fraction of values of $Q^2$ and $R^2_Y$ from permuted data that are greater than or equal to the values from the real data


$$
p = \frac{1 + \#(Q^2_{p} \leq Q^2)}{N}
$$


## RMSEP
I'm now wondering if I did this correctly...
https://stats.stackexchange.com/questions/85507/what-is-the-rmse-of-k-fold-cross-validation

What I did was calculate RMSEP for each fold and then just take the mean of them all.  But the link above seems to indicate I can't just take the mean across all folds for some reason.

So, I first created the splits. The data is split into 7 pieces and for each CV instance, one of those pieces is randomly left out to build the model.  That missing piece is then used to calculate RMSE

$$
RMSE_j = \sqrt{\frac{\sum_i(y_{ij}-\hat{y}_{ij})^2}{N_j}}
$$
where $\hat{y}_{ij}$ is the estimation of $y_{ij}$ and $N_j$ is the number of observations of CV instance $j$.

Then, I just took the mean of these for all of the RMSEjs `r emo::ji("shrug")`

The stack exchange link I found just now says I need to take the square root only *after* averaging the mean squared errors across all folds

$$
\sqrt{\frac{\sum_jMSE_j}{k}}
$$

Or just grouping all the squared errors from all the CV instances together before taking the mean and square root.

$$
\sqrt{\frac{1}{n}\sum_j\sum_k(y_{jk}-\hat{y}_{jk})^2}
$$

