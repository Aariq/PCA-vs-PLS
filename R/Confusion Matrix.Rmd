---
title: "Confusion Matrix Calculations"
output: html_notebook
---
```{r}
library(psych) # for cohen.kappa
library(tidyverse)
library(ropls)
library(here)
library(chemhelper)
```

This notebook is an exploratory look at confusion matrices and Cohen's Kappa coefficient with my simulated data scenarios in mind.  Analysis of the data happens in "Simulated Data Analysis.Rmd"

# Confusion Matrixes

To do this, I employ a confusion matrix/kappa coefficient method.  A confusion matrix is just a contingency table with the intended categories of variables on one side, and the actual classification of the variables by some model on the other side.

E.g.:

```{r echo=FALSE}
df <- tribble(~detected.as, ~actual, ~freq,
              "discriminating", "discriminating", 15,
              "discriminating", "uncorrelated", 2,
              "uncorrelated", "uncorrelated", 10,
              "uncorrelated", "discriminating", 3)
xtabs(freq ~ detected.as + actual, data = df) %>% addmargins()
```

We have 15 true positives in the upper left, 10 true negatives in the lower right, and then 2 false positives in the upper right, and 3 false negatives in the lower left.

## Kappa coefficient

Using the confusion matrix, we can calculate Cohen's kappa which summarizes the ability of a model to classify things correctly.

Cohen's kappa is defined as:

$$
\kappa = \frac{ p_o - p_e}{1-p_e}
$$

where $p_o$ is observed proportionate agreement:

$$
p_o = \frac{\text{true positives} + \text{true negatives}}{\text{total}}
$$
In my above example, $p_o = (15+10)/30$ or 0.8333

and $p_e$ is probability of random agreement:

$$
p_e = (p_{(\text{detected discr})}\times p_{(\text{actual discr})}) + (p_{(\text{detected uncorr})} \times p_{(\text{actual uncorr})})
$$
That is, it uses the row and column summs from the contingency table to calculate probabilities.  For example, 17/30 variables were detected as discriminating, so $p_{\text{(detected discr)}} = 0.5666$

```{r}
pe = (17/30 * 18/30) + (12/30 * 13/30)
pe
```

So K = 

```{r}
K = (0.8333333 - pe)/(1-pe)
K
```

# Applying to PLS-DA and PCA

In order to apply this to PLS-DA, I'll use the common cuttoff of VIP > 1 to indicate whether a variable is discriminating or not.  The name of the variable contains information on whether it was meant to be discriminating or not, so I can then create a confusion matrix and calculate K

It's more difficult to decide how to do this for PCA since there are several ways to go about it.  Often correlation plots are used which plot loadings of the first two PCs with a circle representing a correlation coefficient cutoff, usually of 0.5.  Variables with correlations greater than 0.5 are considered significantly correlated with either PC1, PC2 or both, depending on the location of the variable in loading space.


```{r}
needle_in_haystack <- read_rds(here("data", "needle.rds"))
test <- needle_in_haystack[[sample(1:100, 1)]]

#PCA
test.pca <- opls(select(test, -group), plotL = FALSE, printL = FALSE)

test.scores <-
  test.pca %>% 
  get_plotdata() %>% 
  .$plot_data

test.cors <-
  cor(test.scores[2:3], select(test, -group)) %>%
  t() %>% 
  as.data.frame() %>%
  rownames_to_column("Variable") %>% 
  rowwise() %>% 
  mutate(distance = sqrt(sum(
    (c(p1, p2) - c(0, 0))^2
    ))) %>% 
  mutate(detect_discr = ifelse(distance > 0.5, TRUE, FALSE),
         is_discr = ifelse(str_detect(Variable, "discr"), TRUE, FALSE))

test.cors
conf.pca <-
  table(detected_discr = test.cors$detect_discr, is_discr =test.cors$is_discr)
conf.pca %>% addmargins() # DON'T SAVE IT WITH MARGINS, IT FUCKS UP THE KAPPA COEFFICIENT
```
Kappa:
```{r}
K.pca = cohen.kappa(conf.pca)
K.pca$kappa #I checked this by hand, it's correct.
```

Let's see how PLS-DA does on the same dataset

```{r}
test.pls <- opls(select(test, -group), test$group, plotL = FALSE, printL = FALSE)

test.VIPs <-
  test.pls %>%
  get_VIP() %>% 
  mutate(detect_discr = ifelse(VIP > 1, "discr", "uncorr"),
         is_discr = ifelse(str_detect(Variable, "discr"), "discr", "uncorr"))
conf.pls <- 
  table(detected = test.VIPs$detect_discr, actual = test.VIPs$is_discr)
conf.pls %>% addmargins()
```

Kappa: 

```{r}
K.pls <- cohen.kappa(conf.pls)
K.pls$kappa
```

So PLS-DA obviously does a better job in this dataset.

# Conclusion

This stuff works.  I'm going to apply it to all the datasets in another, less wordy notebook.
