---
title: "Stress testing PCA, RDA, and PLS"
author: "Eric R. Scott"
output: 
  html_notebook: 
    code_folding: hide
    toc: yes
---

# To-do

- change kappa funciton to accept `true=`, and `predicted=` instead of `colA` and `colB`
- change kappa function to not use `dplyr` and calculate more intermediates (improve readability)
- Calculate kappa for all scenarios
- Adapt all scenarios to use new functions
- Plot kappa distributions
- Decide on appropriate cut-off criteria for detecting variable importance in PCA and RDA


# Introduction

The purpose of this notebook is to stress-test PCA, RDA, and PLS-DA to better understand their properties.  All the datasets in this notebook have 35 variables and 20 observations split into two groups ("a" and "b"). I randomly generate `nperm` number of data frames using the same parameters, then obtain p-values from each statistical method on all data sets to test if the groups differ.  I then visualize the distribution of p-values.

# Packages needed

```{r message=FALSE, warning=FALSE}
library(chemhelper) # for sim_*() functions
library(tidyverse)
library(ropls) # I'm open to using pls or another package to fit PLS-DA models.
library(iheatmapr) # for interactive correlation heatmaps
# library(broom)
library(vegan) # for rda()
```

# Define Functions

## Kappa coefficient

Cohen's kappa is defined as:

$$
\kappa = \frac{ p_o - p_e}{1-p_e}
$$

where $p_o$ is observed proportionate agreement and $p_e$ is probability of random agreement.

$$
p_o = \frac{agreements}{total}
$$

$$
p_e = (p_{yesA}\times p_{yesB}) + (p_{noA} \times p_{noB})
$$
```{r}
k.conf <- function(df, colA, colB){
  
  colA <- enexpr(colA)
  colB <- enexpr(colB)
  
  stopifnot(is.logical(df[[colA]]) & is.logical(df[[colB]]))

  conf.df <- df %>% 
    mutate(a = !.data[[colA]] & !.data[[colB]], #both F
           b = .data[[colA]] & !.data[[colB]], #one F one T
           c = !.data[[colA]] & .data[[colB]], #one T one F
           d = .data[[colA]] & .data[[colB]]) %>% #both T
    select(a, b, c, d) %>% 
      summarize(a = sum(a), b = sum(b), c = sum(c), d = sum(d), n = n())
   
  result <- conf.df %>%
    mutate(po = (a+d)/n, #proportion agreement
           pe = (((a + b)/n) * (a + c)/n) + (((c + d)/n) * ((b + d)/n)),
           k = (po - pe) / (1 - pe))
  return(result[["k"]])
}
```

## PCA t-test

Does t-test on PC1 scores, returns p-value.

```{r}
PCA.t.test <- function(pca.m, group){
  pca.d <- get_plotdata(pca.m)$plot_data
  pca.t <- t.test(pca.d$p1 ~ group)
  return(pca.t$p.value)
}
```

## RDA ANOVA

Fits RDA model, runs ANOVA for global significance of model, returns p-value.

```{r}
RDA.anova <- function(rda.m){
  return(anova(rda.m)$`Pr(>F)`[[1]])
}
```

## PLS-DA

Fits PLS-DA model, calculates p-values with 100 permutations, returns $Q^2$, $p_{(R^2_Y)}$, and $p_{(Q^2)}$

```{r}
PLS.test <- function(pls.m){
  return(select(pls.m@summaryDF, `Q2(cum)`, pR2Y, pQ2))
}
```



# 1. Many, highly multicollinear distinguishing variables contributing to large effect size.

This code creates a dataset with 30 discriminating variables that co-vary and 5 uncorrelated variables.

```{r}
base_df <- sim_df(20, 2)
nperm = 50
```

```{r}
df1 <- map(1:nperm,
    ~base_df %>% 
      sim_covar(5, var = 1, cov = 0, name = "uncorr") %>% 
      sim_discr(30, var = 1, cov = 0.5, group_means = c(-0.5, 0.5), name = "discr")
) %>% set_names(paste("dataset", 1:nperm))

df1[[1]] %>% 
  select(-group) %>% 
  cor() %>% 
  iheatmap(row_labels = TRUE, col_labels = TRUE, name = "cor")
```

**PCA 1**

```{r}
pca1 <- map(df1, ~opls(select(., -group), plotL = FALSE, printL = FALSE))
pvals.pca1 <- map_dbl(pca1, ~PCA.t.test(., group = df1[[1]]$group))
```

Try calculating kappa coeffiecients

```{r}
ks.pca1 <- pca1 %>% map(~get_loadings(.) %>%
               mutate(is.discr = str_detect(Variable, "discr"),
                      pred.discr = p1 > 0.15)) %>% 
  map_dbl(~k.conf(., "is.discr", "pred.discr"))
mean(ks.pca1)
```

**RDA 1**

```{r}
rda1 <- map(df1, ~rda(select(., -group) ~ .$group))
pvals.rda1 <- map_dbl(rda1, RDA.anova)
```

Kappa coefficient
```{r}
library(RVAideMemoire)
ks.rda1 <- map(rda1, ~scores(.)$species %>%
      as.data.frame() %>%
      rownames_to_column(var = "Variable") %>% 
      mutate(is.discr = str_detect(Variable, "discr"),
             pred.discr = abs(RDA1) > 0.15)) %>%
  map_dbl(~k.conf(., "is.discr", "pred.discr"))
mean(ks.rda1)
```


**PLS 1**

```{r}
pls1 <- df1 %>% 
  map(~opls(select(., -group), .$group, permI = 100, predI = 2, printL = FALSE, plotL = FALSE))

output.pls1 <- map(pls1, PLS.test) %>% bind_rows()
```

Kappa coefficient

```{r}
ks.pls1 <- pls1 %>% map(~get_VIP(.) %>% 
  mutate(is.discr = str_detect(Variable, "discr"), pred.discr = VIP > 1)) %>% 
  map_dbl(~k.conf(., "is.discr", "pred.discr"))
mean(ks.pls1)
```


## Results 1

```{r}
results1 <- output.pls1 %>% add_column(pvals.rda1, pvals.pca1)
results1 %>% summarize_all(funs(mean = mean))
```

```{r}
pvals1 <- results1 %>% 
  select(`PLS pQ2` = pQ2, `RDA ANOVA` = pvals.rda1, `PCA t-test` = pvals.pca1) %>% 
  gather(key = "Method", value = "p value")

ggplot(pvals1, aes(x = `p value`, fill = Method)) +
  # geom_density(alpha = 0.3) +
  geom_histogram(bins = 20, position = "dodge") +
  geom_vline(aes(xintercept = 0.05), color = "red")

```

In this scenario, PLS gives a low p-value.  Since all 30 of the discriminating variables covary strongly with eachother, I interpret this as PLS being highly robust to multicollinearity.  That is, there aren't *really* 30 discriminating variables, but one latent variable that affects all 30 variables.  I imagine that if I were to create 30 discriminating variables in 6 groups of 5 variables that the p-values of all methods would be similar.

# 2. Needle in a haystack---many, highly collinear variables, few discriminating variables

```{r}
df2 <- map(1:nperm, ~base_df %>% 
  sim_covar(10, 1, 0.5, "corrA") %>% 
  sim_covar(10, 1, 0.5, "corrB") %>% 
  sim_covar(10, 1, 0.5, "corrC") %>% 
  sim_discr(5, 1, 0, group_means = c(-1, 1), name = "discr")
  )

df2[[2]] %>% 
  select(-group) %>% 
  cor() %>% 
  iheatmap(row_labels = TRUE, col_labels = TRUE, name = "cor")
```

**PCA 2**

```{r}
pvals.pca2 <- map_dbl(df2, PCA.t.test)
mean(pvals.pca2)
```

**RDA 2**

```{r}
pvals.rda2 <- map_dbl(df2, RDA.anova)
mean(pvals.rda2)
```

**PLS 2**

```{r}
output.pls2 <- map(df2, PLS.test) %>% bind_rows()
```

## Results 2

```{r}
results2 <- output.pls2 %>% add_column(pvals.rda2, pvals.pca2)
results2 %>% summarize_all(funs(mean = mean))
```

```{r}
pvals2 <- results2 %>% 
  select(`PLS pQ2` = pQ2, `RDA ANOVA` = pvals.rda2, `PCA t-test` = pvals.pca2) %>% 
  gather(key = "Method", value = "p value")

ggplot(pvals2, aes(x = `p value`, fill = Method)) +
  # geom_density(alpha = 0.3) + 
  geom_histogram(position = "dodge", bins = 20) +
  # ylim(0, 20) +
  # xlim(0, 0.5) +
  geom_vline(aes(xintercept = 0.05), color = "red")
```
Here PCA does less well because the variables that contribute most to variation in the dataset are not those variables that contribute to differences between groups. PLS-DA and RDA both consistently find differences between the groups.  However, these significant differences are only due to **5** variables.  It's important to note how much variation is explained by the predictive axes of PLS or the constrained portion of RDA to interpret these results.

# 3. Red Herring? (AKA Needle in a highly multicollinear haystack.)

Ok, this is ridiculous, but here is a dataset with 30, strongly multicollinear variables that don't discriminate between groups and only 5 variables that do discriminate between groups.

```{r}
df3 <- map(1:nperm,
             ~base_df %>% 
               sim_covar(30, 1, 0.7, name = "corr") %>% 
               sim_discr(5, 1, 0, name = "discr", group_means = c(-0.5, 0.5))
)

df3[[1]] %>% 
  select(-group) %>% 
  cor() %>% 
  iheatmap(row_labels = TRUE, col_labels = TRUE, name = "cor")
```

**PCA 3**

```{r}
pvals.pca3 <- map_dbl(df3, PCA.t.test)
mean(pvals.pca3)
```

**RDA 2**

```{r}
pvals.rda3 <- map_dbl(df3, RDA.anova)
mean(pvals.rda3)
```

**PLS 2**

```{r}
output.pls3 <- map(df3, PLS.test) %>% bind_rows()
```

## Results 3

```{r}
results3 <- output.pls3 %>% add_column(pvals.rda3, pvals.pca3)
results3 %>% summarize_all(funs(mean = mean))
meanQ <- mean(results3$`Q2(cum)`)
```

```{r}
pvals3 <- results3 %>% 
  select(`PLS pQ2` = pQ2, `RDA ANOVA` = pvals.rda3, `PCA t-test` = pvals.pca3) %>% 
  gather(key = "Method", value = "p value")

ggplot(pvals3, aes(x = `p value`, fill = Method)) +
  # geom_density(alpha = 0.3) + 
  geom_histogram(position = "dodge", bins = 20) +
  geom_vline(aes(xintercept = 0.05), color = "red")
```

Even in this ridiculous scenario, PLS is able to "find" the 5 discriminating variables most of the time and give a significant p-value.  The mean $Q^2$ value is `r round(meanQ, 2)`, so it's possible that some of those "significant" p-values were obtained from models with poor predictive power.  Because within group co-variation is high and between-group co-variation is low, the F-test used to analyze RDA results gives a high p-value.  I think. 


